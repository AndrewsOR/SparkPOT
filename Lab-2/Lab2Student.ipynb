{"cells": [{"cell_type": "markdown", "source": "# Introduction to Apache Spark lab, Lab 2: Querying data\n![image](https://hadoopi.files.wordpress.com/2014/10/screen-shot-2014-10-25-at-14-29-50.png?w=597&h=222)\n\nThis lab will show you how to work with the [SparkSQL](http://spark.apache.org/sql/) library.  It's meant to be self-guided, but don't hesitate to ask your presentor for help.  \n\nThis notebook guides you through querying data with Apache Spark, including how to create and use DataFrames, run SQL queries, apply functions to the results of SQL queries, join data from different data sources, and visualize data in graphs.\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 2.0.", "metadata": {}}, {"cell_type": "markdown", "source": "## Table of contents\n\n1. [Prepare the environment and the data](#getstarted)<br>\n     1.1 [Enable SQL processing](#sqlprocessing)<br>\n     1.2 [Download the data](#download)<br>\n     1.3 [Create a DataFrame](#createdf)<br>\n     1.4 [Create a table](#createtab)<br>\n2. [Run SQL queries](#runsql)<br>\n    2.1 [Display query results with a pandas DataFrame](#pandas)<br>\n    2.2 [Run a group by query](#groupby)<br>\n    2.3 [Run a subselect query](#subselect)<br>\n    2.4 [Return nested JSON field values](#nested)<br>\n3. [Convert RDDs to DataFrames](#convertrdd)<br>\n    3.1 [Create a simple RDD](#simplerdd)<br>\n    3.2 [Apply a schema](#apply)<br>\n    3.3 [Create rows with named columns](#namedcol)<br>\n    3.4 [Join tables](#join)<br>\n4. [Create SQL functions](#sqlfuncs)<br>\n5. [Convert a pandas DataFrame to a Spark DataFrame](#sparkdf)<br>\n    5.1 [Get a new data set](#ufo)<br>\n    5.2 [Create a pandas DataFrame](#ufopandas)<br>\n    5.3 [Convert to a Spark DataFrame](#sparkufo)<br>\n    5.4 [Run an SQL statement](#runufo)<br>\n6. [Visualize data](#viz)<br>\n    6.1 [Create a chart](#vizchart)<br>\n    6.2 [Aggregate the data](#vizagg)<br>\n    6.3 [Create a better chart](#vizchart2)<br>\n7. [Reading from an external source](#external)<br>\n8. [Summary and next steps](#nextsteps)", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"getstarted\"></a>\n## 1. Getting started\nBefore you can run SQL queries on data in an Apache Spark environment, you need to enable SQL processing and then move the data to the structured format of a DataFrame\n<a id=\"sqlprocessing\"></a>\n## 1.1 Create a [Spark Session](http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sparksession)\nThe preferred method to enable SQL processing with Spark 2.0 is to use the new SparkSession object, but you can also create a SQLContext object. <br>\n<br/>\n <div class=\"panel-group\" id=\"accordion-11\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><i>SparkSession</i> is not included by default.   You need to import it from <i>pyspark.sql</i></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse2-11\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><i>SparkSession</i> needs the builder method.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse3-11\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br/>\n\nfrom pyspark.sql import SparkSession<br/>\nspark = SparkSession.builder.getOrCreate()<br/></div>\n    </div>\n  </div>\n</div> \n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "# Import the SparkSQL library and connect to the current Spark context\n"}, {"cell_type": "markdown", "source": "<a id=\"download\"></a>\n## 1.2 - Download a JSON Recordset to work with\nLet's download the data, we can run commands on the console of the server (or docker image) that the notebook environment is using. To do so we simply put a \"!\" in front of the command that we want to run. For example:\n\n!pwd\n\nTo get the data we will download a file to the environment. Simple run these two commands, the first just ensures that the file is removed if it exists:\n\n!rm world_bank.json.gz -f <br/>\n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz<br/><br/>\n <div class=\"panel-group\" id=\"accordion-12\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-12\" href=\"#collapse1-12\">\n        Advanced Optional 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-12\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Comment out the rm statement i.e. #!rm and re-run this section.   What is the name of the downloaded file?</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-12\" href=\"#collapse2-12\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-12\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Add !ls to see all the files currently in storage.   Try running !mkdir testdir</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-12\" href=\"#collapse3-12\">\n        Advanced Optional 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-12\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Clean up all added files/directories.   Use !rmdir to remove a directory.</div>\n    </div>\n  </div>\n</div> \n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "# Download file here\n!rm world_bank.json.gz* -f\n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"}, {"cell_type": "markdown", "source": "<a id=\"createdf\"></a>\n## 1.3 - Create a Dataframe \n<br/>\nUse the SparkSession you created earlier to read the World Bank json data - <i>world_bank.json.gz</i> as a Dataframe</a><br><br>\n <div class=\"panel-group\" id=\"accordion-13\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse1-13\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-13\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>read</i> function to return a Dataframe reader</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse2-13\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-13\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>json()</i> method in DataframeReader to read the file.   Note that the method handles a gzipped file format.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse3-13\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-13\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">To create the Dataframe type:<br>\n\nexample1_df = spark.read.json(\"world_bank.json.gz\")<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-13\" href=\"#collapse4-13\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-13\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Obtain the same result by using <i>textFile()</i> to read the file as RDD and then convert to a Dataframe</div>\n    </div>\n  </div>\n</div> \n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "#Create the Dataframe here:\n\n\n#Advanced Solution\n"}, {"cell_type": "markdown", "source": " ### 1.3.1 - Show the Dataframe schema\n <br>\n <div class=\"panel-group\" id=\"accordion-131\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-131\" href=\"#collapse1-131\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-131\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><h3>We can look at the schema with this command:</h3>\n\nType: <br>\nexample1_df.printSchema()</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-131\" href=\"#collapse2-131\">\n        Advanced Optional 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-131\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Get the dataframe columns.   Try using command-completion (use TAB after the .) to obtain the list of possible methods/values</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-131\" href=\"#collapse3-131\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-131\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Convert the dataframe back to JSON and print the first value</div>\n    </div>\n  </div>\n</div>", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "#Print out the schema\n\n#Advanced Option 1\n\n\n#Advanced Option 2\n"}, {"cell_type": "markdown", "source": "### 1.3.2 - Using the Dataframe\n<br/>\nDataframes are a subset of RDDs and can be similarly transformed.  You can map and filter them.\n<br/>Take a look at the first two rows of data using the [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=take#pyspark.sql.DataFrame.take) function.<br/>\n<div class=\"panel-group\" id=\"accordion-132\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-132\" href=\"#collapse1-132\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-132\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">example1_df.take(2)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-132\" href=\"#collapse2-132\">\n        Advanced Optional 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-132\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><i>take()</i> returns data as an RDD list of Row objects.   <i>show()</i> prints the objects to the console.   What is the default number of rows displayed?</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-132\" href=\"#collapse3-132\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-132\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Save the table as a parquet table.   Use !ls to confirm it was saved.  Use a <i><a href=\"https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\"DataFrameWriter></a></i>  What did you see when you ran the ls command?</div>\n    </div>\n  </div>\n</div> \n \n\n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "#Use take on the DataFrame to pull out 2 rows\n\n\n#Advanced Option 1\n\n\n#Advanced Option 2\n"}, {"cell_type": "markdown", "source": "<a id=\"createtab\"></a>\n## 1.4 - Register a Temp Table\n<br/>\nSQL works on tables.   Currently we have data in a dataframe, but we have no table identifier for it.   Thus, we want to create a temporary table reference that refers to this dataframe.\n<br/><br/>\n<div class=\"panel-group\" id=\"accordion-14\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse1-14\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">The function is: DataframeObject.createTempView(\"name_of_table\")<br>\nCreate a table named \"world_bank\"<br/></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse2-14\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">example1_df.createTempView(\"world_bank\")</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse3-14\">\n        Advanced Optional 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">You'll need to create a [catalog](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=catalog#pyspark.sql.SparkSession.catalog) object</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse4-14\">\n        Advanced Optional 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Try creating a second temporary table on the same dataframe.   What does <i>listTables()</i> return?</div>\n    </div>\n  </div>\n    <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse5-14\">\n        Advanced Optional 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse5-14\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Drop the additional temp table.   What does <i>listTables()</i> return?</div>\n    </div>\n  </div>\n</div>\n\n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false}, "source": "#Create the table to be referenced via SparkSQL\n\n\n#Advanced Optional 1\n\n\n#Advanced Optional 2\n\n\n#Advanced Optional 2\n"}, {"cell_type": "markdown", "source": "<a id=\"runsql\"></a>\n## 2 - Writing SQL Statements\n<br>\nWrite SQL statements to return two rows from the world_bank table.\n<br>\n <div class=\"panel-group\" id=\"accordion-2\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-2\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>sql()</i> method on your SparkSession</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse2-2\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use <i>limit</i> (i.e. <i>limit 2</i>) within your SQL statement to limit the number of rows returned.   Use <i>show()</i> to display the values.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse3-2\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\n      spark.sql(\"select * from world_bank limit 2\").show()<br></div>\n    </div>\n  </div>\n</div> \n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "#Use SQL to query the table and print the output\n"}, {"cell_type": "markdown", "source": "###  Writing SQL Statements\n<br>\nTry writing the next four sections yourself first.   Each hint contains the solution for that section.   We provide this here because this is more SQL than Spark and not everyone is familar with SQL.  Nor is this an SQL class!\n<br><br>\n <div class=\"panel-group\" id=\"accordion-21\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse1-21\">\n        Solution 2.1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">spark.sql(\"select * from world_bank limit 2\").toPandas()</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse2-21\">\n        Solution 2.2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">spark.sql(\"select regionname, count(&#42;) as regioncount from world_bank group by regionname order by regioncount desc\").toPandas()</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse3-21\">\n        Solution 2.3</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><pre>query = \"\"\"\nselect \\* from\n    (select\n        regionname ,\n        count(\\*) as project_count\n    from world_bank\n    group by regionname \n    order by count(\\*) desc) table_alias\nlimit 2\n\"\"\"\n\nsqlContext.sql(query).toPandas()</pre></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse4-21\">\n        Solution 2.4</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">spark.sql(\"select sector.Name from world_bank limit 5\").toPandas()</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"pandas\"></a>\n## 2.1 - Use Pandas to display the results", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "# Take the DataFrame you created with the two records and convert it into a Pandas DataFrame\n\n"}, {"cell_type": "markdown", "source": "<a id=\"groupby\"></a>\n## 2.2 - Use group by and order by to query the data", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"scrolled": true}, "source": "# Now calculate a simple count based on a group, for example \"regionname\".   Return the regionname and a count of the values for that regionname.\n# optionally order the results in descending order\n\n"}, {"cell_type": "markdown", "source": "<a id=\"subselect\"></a>\n## 2.3 - Use a subselect query ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "# run a subselect query\n# Calculate a count of projects by region using a subselect\n# Encase the query inside of the query string\n# Display the result using Pandas\n\n"}, {"cell_type": "markdown", "source": "<a id=\"nested\"></a>\n## 2.4 - Use the JSON schema to select based on a nested JSON column", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "# With JSON data you can reference the nested data.  \n# If you look at the Schema in Step 1.3.1 above you can see that sector.Name is a nested column.\n# Select that column and limit to a reasonable output (say five rows)\n\n"}, {"cell_type": "markdown", "source": "<a id=\"convertrdd\"></a>\n## 3. Convert RDDs to DataFrames\nIf you want to run SQL queries on an existing RDD, you must convert the RDD to a DataFrame. The main difference between RDDs and DataFrames is whether the columns are named.\n\nYou'll create an RDD and then convert it to a DataFrame in two different ways:\n - [Apply a schema](#apply)\n - [Create rows with named columns](#namedcol)", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"simplerdd\"></a>\n### 3.1 Create a simple RDD\nYou'll create a simple RDD with an ID column and two columns of random numbers.\n\nFirst create a Python list of lists:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "import random\n\ndata_e2 = []\nfor x in range(1,6):\n    random_int = int(random.random() * 10)\n    data_e2.append([x, random_int, random_int^2])\n    \n"}, {"cell_type": "markdown", "source": "Now create the RDD and display it:\n<br/>\n<div class=\"panel-group\" id=\"accordion-31\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-31\" href=\"#collapse1-31\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-31\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">sc.parallelize()</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-31\" href=\"#collapse2-31\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-31\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Parallelized objects need to be collected.   Use collect()</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-31\" href=\"#collapse3-31\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-31\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">rdd_example2 = sc.parallelize(data_e2)<br>\nprint rdd_example2.collect()</div>\n    </div>\n  </div>\n</div> \n ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": ""}, {"cell_type": "markdown", "source": "<a id=\"apply\"></a>\n### 3.2 Apply a schema\nYou'll use the `StructField` method to create a schema object that's based on a string, apply the schema to the RDD to create a DataFrame, and then create a table to run SQL queries on.\n\nDefine your schema columns as a string:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "from pyspark.sql.types import *\n\nschemaString = \"ID VAL1 VAL2\""}, {"cell_type": "markdown", "source": "Assign header information with the `StructField` method and create the schema with the `StructType` method:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)"}, {"cell_type": "markdown", "source": "Apply the schema to the RDD with the createDataFrame() method and save it to the schemaExample variable:\n<br/>\n<div class=\"panel-group\" id=\"accordion-32\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse1-32\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">createDataFrame() can be invoked from the SparkSession context</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse2-32\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">createDataFrame(rdd,schema)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse3-32\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">schemaExample = spark.createDataFrame(rdd_example2, schema)</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ""}, {"cell_type": "markdown", "source": "Register the DataFrame as a table:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "schemaExample.registerTempTable(\"example2\")"}, {"cell_type": "markdown", "source": "View the data:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "print schemaExample.collect()"}, {"cell_type": "markdown", "source": "You can reference the columns names in DataFrames", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "for row in schemaExample.take(2):\n    print row.ID, row.VAL1, row.VAL2"}, {"cell_type": "markdown", "source": "Run a simple SQL query:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "sqlContext.sql(\"select * from example2\").toPandas()"}, {"cell_type": "markdown", "source": "<a id=\"namedcol\"></a>\n### 3.3 Create rows with named columns\nYou'll create an RDD with named columns and then convert it to a DataFrame and a table.\n\nCreate a new RDD and specify the names of the columns with the `map` method:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "from pyspark.sql import Row\n\nrdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n\nprint rdd_example3.collect()            "}, {"cell_type": "markdown", "source": "Convert `rdd_example3` to a DataFrame and register an associated table:\n<br/>\n<div class=\"panel-group\" id=\"accordion-33\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse1-33\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">toDF() can be invoked on an RDD to convert it to a dataframe</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse2-33\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">registerTempTable(<name>) is used to register temp tables</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse3-33\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">df_example3 = rdd_example3.toDF()<br>\ndf_example3.registerTempTable(\"example3\")</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse3-34\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Display the list of tables using the catalog attribute on the spark object</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": ""}, {"cell_type": "markdown", "source": "Run a simple SQL query:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "sqlContext.sql(\"select * from example3\").toPandas()"}, {"cell_type": "markdown", "source": "<a id=\"join\"></a>\n### 3.4 Join tables\nYou can join tables.\n\nJoin tables `example2` and `example3` on the ID column:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "query = \"\"\"\nselect\n    *\nfrom\n    example2 e2\ninner join example3 e3 on\n    e2.ID = e3.id\n\"\"\"\n\nprint sqlContext.sql(query).toPandas()"}, {"cell_type": "markdown", "source": "Alternatively, you can join DataFrames with a Dataframe API instead of an SQL query:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "df_example4 = df_example3.join(schemaExample, schemaExample[\"ID\"] == df_example3[\"id\"] )\n\nfor row in df_example4.take(5):\n    print row"}, {"cell_type": "markdown", "source": "<a id=\"sqlfuncs\"></a>\n## 4. Create SQL functions \nYou can create functions that run in SQL queries. \n\nFirst, create a Python function and test it:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "def simple_function(v):\n    return int(v * 10)\n\n#test the function\nprint simple_function(3)"}, {"cell_type": "markdown", "source": "Next, register the function as an SQL function with the registerFunction() method - in this case we do need to use sqlContext since the SparkSession object does not support this function:\n<br/>\n<div class=\"panel-group\" id=\"accordion-4\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse1-4\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-4\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">registerFunction() is invoked from the SQL Context</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse2-4\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-4\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">registerFunction(function name, function)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse3-4\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-4\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">sqlContext.registerFunction(\"simple_function\", simple_function)</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "\n"}, {"cell_type": "markdown", "source": "Now run the function in an SQL Statement:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "query = \"\"\"\nselect\n    ID,\n    VAL1,\n    VAL2,\n    simple_function(VAL1) as s_VAL1,\n    simple_function(VAL2) as s_VAL2\nfrom\n example2\n\"\"\"\nspark.sql(query).toPandas()"}, {"cell_type": "markdown", "source": "The values in the VAL1 and VAL2 columns look like strings (10 characters instead of a number multiplied by 10). That's because string is the default data type for columns in Spark DataFrames.\nCast the values in the VAL1 and VAL2 columns to integers:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "query = \"\"\"\nselect\n    ID,\n    VAL1,\n    VAL2,\n    simple_function(cast(VAL1 as int)) as s_VAL1,\n    simple_function(cast(VAL2 as int)) as s_VAL2\nfrom\n example2\n\"\"\"\nsqlContext.sql(query).toPandas()"}, {"cell_type": "markdown", "source": "That looks better!", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"sparkdf\"></a>\n## 5. Convert a pandas DataFrame to a Spark DataFrame\nAlthough pandas DataFrames display data in a friendlier format, Spark DataFrames can be faster and more scalable.\n\nYou'll get a new data set, create a pandas DataFrame for it, and then convert the pandas DataFrame to a Spark DataFrame.", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"ufo\"></a>\n### 5.1 Get a new data set\nGet a data set about UFO sightings:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "#!rm SIGHTINGS.csv -f\n#!wget https://www.quandl.com/api/v3/datasets/NUFORC/SIGHTINGS.csv\n!rm SIGHTINGS.csv\n!wget https://raw.githubusercontent.com/jpatter/SparkPOT/master/Lab-2/data/SIGHTINGS.csv\n"}, {"cell_type": "markdown", "source": "<a id=\"ufopandas\"></a>\n### 5.2 Create a pandas DataFrame\nCreate a pandas DataFrame of the data set with the `read_csv` method:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "import pandas as pd\n\npandas_df = pd.read_csv(\"./SIGHTINGS.csv\")\npandas_df.head()"}, {"cell_type": "markdown", "source": "<a id=\"sparkufo\"></a>\n### 5.3 Convert to a Spark DataFrame\nConvert the pandas DataFrame to a Spark DataFrame with the `createDataFrame` method. Remember using the `createDataFrame` method to convert an RDD to a Spark DataFrame?", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "spark_df = spark.createDataFrame(pandas_df)"}, {"cell_type": "markdown", "source": "Print the first two rows:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "for row in spark_df.take(2):\n    print row"}, {"cell_type": "markdown", "source": "Register the Spark DataFrame as a table:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "spark_df.registerTempTable(\"ufo_sightings\")"}, {"cell_type": "markdown", "source": "<a id=\"runufo\"></a>\n### 5.4 Run an SQL statement\nNow run an SQL statement to print the first 10 rows of the table:\n<br/>\n<div class=\"panel-group\" id=\"accordion-54\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-54\" href=\"#collapse1-54\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-54\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">SQL statements can be run from the spark object</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-54\" href=\"#collapse2-54\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-54\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">print spark.sql(\"select * from ufo_sightings limit 10\")</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-54\" href=\"#collapse3-54\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-54\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">print spark.sql(\"select * from ufo_sightings limit 10\").collect()</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": ""}, {"cell_type": "markdown", "source": "<a id=\"viz\"></a>\n## 6. Visualize data\nIt's easy to create charts from pandas DataFrames. You'll use the matplotlib library to create graphs and the NumPy package for computing.\n\nImport the libraries and specify to show graphs inline:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "%matplotlib inline \nimport matplotlib.pyplot as plt, numpy as np"}, {"cell_type": "markdown", "source": "Convert the Spark DataFrame with UFO data to a pandas DataFrame:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "ufos_df = spark_df.toPandas()"}, {"cell_type": "markdown", "source": "<a id=\"vizchart\"></a>\n### 6.1 Create a chart\n\nTo create a chart, you call the `plot()` method and specify the type of chart, the columns for the X and Y axes, and, optionally, the size of the chart. \n\nFor more information about plotting pandas DataFrames, see [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html).\n\nCreate a bar chart 12\" wide by 5\" high that shows the number of reports by date:\n<br/>\n<div class=\"panel-group\" id=\"accordion-61\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-61\" href=\"#collapse1-61\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-61\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">plot() is run from the dataframe you wish to use</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-61\" href=\"#collapse2-61\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-61\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">ufos_df.plot(kind='',x='',y='',figsize=())</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-61\" href=\"#collapse3-61\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-61\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))</div>\n    </div>\n  </div>\n</div> ", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": ""}, {"cell_type": "markdown", "source": "This chart doesn't look good because there are too many observations. Check how many observations there are by querying the ufo_sightings table:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "print sqlContext.sql(\"select count(*) from ufo_sightings\").collect()"}, {"cell_type": "markdown", "source": "## <a id=\"vizagg\"></a>\n### 6.2 Aggregate the data\n\nTo reduce the number of data points on the chart, you can aggregate the data by year. Here are a few of the ways that you can do that:\n\n - Run an SQL query on the Reports column to output the year, and then run a group by operation on the year.\n - Create a simple Python function to aggregate by year, and then run the function in an SQL query.\n - Run the `map()` method on the Spark Dataframe to append a new column that contains the aggregated count by year. This is the method you'll use.\n\nRemember that the dates in the Reports column look like this: 2/28/2017. Therefore, to create the year column, you just need the last 4 characters from the Reports column.\n\nAdd a year column to the DataFrame:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "ufos_df = spark_df.rdd.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[-9:-5])))).toDF()"}, {"cell_type": "markdown", "source": "Check to verify that you get the expected results:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "print ufos_df.take(5)"}, {"cell_type": "markdown", "source": "Register the DataFrame as a table:", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": "ufos_df.registerTempTable(\"ufo_withyear\")"}, {"cell_type": "markdown", "source": "<a id=\"vizchart2\"></a>\n### 6.3 Create a better chart\n\nNow run an SQL query to group by year, order by year, and filter to the last 66 years. Then create a pandas DataFrame for the results and create a chart of the number of reports by year.", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "query = \"\"\"\nselect \n    sum(Count) as count, \n    year \nfrom ufo_withyear\nwhere year > 1950\ngroup by year\norder by year\n\"\"\"\npandas_ufos_withyears = sqlContext.sql(query).toPandas()\npandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))"}, {"cell_type": "markdown", "source": "Now you have a chart that you can read. Look back at the original chart and notice that it wasn't ordered ascending by year.", "metadata": {}}, {"cell_type": "markdown", "source": "<a id=\"external\"></a>\n## 7 - Reading from an external data source\nIf you have time, this is a good example to show you how to read from other datasources.  <br><br>\nIn a different browser tab, create a dashDB service, add credentials and come back to this notebook. <br>If you are using Data Science Experience, you need to log into Bluemix and create a dashDB instance.   The login and password should be the same as for DSE.<br>\nEach dashDB instance in Bluemix is created with a \"GOSALES\" set of tables which we can reuse for the purpose of this example.  For example GOSALES.BRANCH. (You can create your own tables if you wish...)<br><br>Replace the Xs in the cell below with proper credentials and verify access to dashDB tables.<br><br>\nYou can read from any database that you can connect to through jdbc.  Here is the [documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases)\n<br><br>\n <div class=\"panel-group\" id=\"accordion-8\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-8\" href=\"#collapse1-8\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-8\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">To connect to a general dashDB instance:<br>\n      host=\"\"<br>\nuser=\"\"<br>\npassword=\"\"<br>\nconnection=\"jdbc:db2://\" + host + \":50000/BLUDB:user=\" + user + \";password=\" + password + \";\"<br></div>\nwhere<br>\nuser=dash14429<br>\npassword=}oTp-6pPDC4v<br>\nhost=dashdb-entry-yp-dal09-10.services.dal.bluemix.net<br>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-8\" href=\"#collapse2-8\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-8\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Create your own dashDB instance in Bluemix and connect to it</div>\n    </div>\n  </div>\n</div> \n", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ""}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {}, "source": "salesDF = spark.read.format('jdbc').\\\n          options(url=connection,\\\n                  dbtable='GOSALES.BRANCH').load()\nsalesDF.toPandas()"}, {"cell_type": "markdown", "source": "<a id=\"nextsteps\"></a>\n## 8. Summary and next steps\nYou've learned how to create DataFrames, convert between DataFrame types, and convert from RDDs. You know how to run SQL queries and create SQL functions. And you can visualize the data in charts. \n\nDig deeper:\n - [Apache Spark documentation](http://spark.apache.org/documentation.html)\n - [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html)\n - [pandas](http://pandas.pydata.org/pandas-docs/stable/index.html)\n - [matplotlib](http://matplotlib.org/)\n - [NumPy](http://www.numpy.org/)", "metadata": {}}, {"cell_type": "markdown", "source": "### Authors\nCarlo Appugliese is a Spark and Hadoop evangelist at IBM.<br/>\nBraden Callahan is a Big Data Technical Specialist for IBM.<br/>\nRoss Lewis is a Big Data Technical Sales Specialist for IBM.<br/>\nMokhtar Kandil is a World Wide Big Data Technical Specialist for IBM.<br/>\nJoel Patterson is a Big Data Technical Specialist for IBM.<br/>\n\n### Data citation\n\nThe World Bank: Projects & Operations ", "metadata": {}}, {"cell_type": "markdown", "source": "<hr>\nCopyright &copy; IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License.", "metadata": {}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ""}], "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python2-spark20", "display_name": "Python 2 with Spark 2.0"}, "language_info": {"version": "2.7.11", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "nbformat": 4}